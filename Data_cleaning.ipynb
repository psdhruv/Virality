{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge all article files(excel) in one dataframe\n",
    "import pandas as pd\n",
    "import glob\n",
    "#making list of strings from txt files in articles folder\n",
    "path = './Articlespk/*xlsx' # path of files\n",
    "files = glob.glob(path)\n",
    "\n",
    "all_articles = pd.DataFrame() #all articles with article id\n",
    "for f in files:\n",
    "    df = pd.read_excel(f)\n",
    "    print(len(df))\n",
    "    all_articles = all_articles.append(df,ignore_index=True, sort=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data extraction from xml(the data provided by client for article content)\n",
    "import lib\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#making list of dictionaries where 1 dictionary= 1 article data\n",
    "def remove_tags(text):\n",
    "    TAG_RE = re.compile(r'<[^>]+>')\n",
    "    temp= TAG_RE.sub('', text)\n",
    "    temp= temp.replace('\\n', \" \")\n",
    "    temp=temp.replace('&nbsp;',' ')\n",
    "    return temp\n",
    "\n",
    "def xml_to_excel(fileurl,excel_name):\n",
    "    tree = ET.parse(fileurl)\n",
    "    root = tree.getroot()\n",
    "    list_of_dict=[]\n",
    "    for article in root:\n",
    "        temp={}\n",
    "        temp[article[0].tag] = article[0].text\n",
    "        temp[article[1].tag] = article[1].text\n",
    "        temp[article[2].tag] = article[2].text\n",
    "        temp[article[4].tag] = article[4].text\n",
    "        temp[article[5].tag] = article[5].text\n",
    "        temp[article[6].tag] = article[6].text\n",
    "        temp[article[7].tag] = article[7].text\n",
    "        #temp[article[9].tag] = article[9].text\n",
    "\n",
    "        list_of_dict.append(temp)\n",
    "\n",
    "    #may_dict = xml_to_dict(\"May2019.xml\")\n",
    "    dic_df = pd.DataFrame.from_dict(list_of_dict, orient='columns')\n",
    "    dic_df[\"dtls\"] = pd.Series([remove_tags(x) for x in dic_df[\"dtls\"]])\n",
    "    dic_df.to_excel(excel_name+'.xlsx')\n",
    "    trans = dic_df[['dtls', 'news_id']]\n",
    "    trans.to_excel(excel_name+'_trans.xlsx')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtering rows and extracting article id example:(it mayt be different depending on your data)\n",
    "#filtering unnecessary rows example\n",
    "GA_list=[]#weekday is data extracted from GA\n",
    "for row in weekday.itertuples(index=False):\n",
    "   if re.findall(\"-\\d*\\d\\d\\d\\d\\d\", row.Page):\n",
    "       GA_list.append(row)\n",
    "weekday_clean=pd.DataFrame(GA_list)\n",
    "ids=[]\n",
    "#extracting ids\n",
    "for row in weekday_clean.itertuples(index=False):\n",
    "    try:\n",
    "        temp=re.findall(\"-\\d*\\d\\d\\d\\d\\d\", row.Page)[0].split(\"-\", 1)[-1]\n",
    "        ids.append(temp)\n",
    "    except:\n",
    "        print(row.Page)\n",
    "ids=pd.Series(ids)\n",
    "weekday_clean[\"news_id\"]=ids\n",
    "del autho_clean[\"Page\"]\n",
    "autho_clean[\"news_id\"]=autho_clean[\"news_id\"].astype(\"int\")\n",
    "autho_clean=autho_clean.drop_duplicates()\n",
    "autho_clean=autho_clean.sort_values([\"news_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import timedelta \n",
    "dts=[datetime.datetime.strptime(str(x),\"%Y%m%d%H\")  for x in GA_clean[\"hourofday\"] ]\n",
    "GA_clean[\"hourofday\"]=pd.Series(dts)\n",
    "GA_xml=pd.merge(GA_clean,all_data[[\"crtd_date\", \"news_id\"]], on= \"news_id\")\n",
    "#crtd date\n",
    "crtd=[datetime.datetime.strptime(x.rsplit(\"+\", 1)[0].rsplit('.',1)[0], '%Y-%m-%dT%H:%M:%S') for x in GA_xml[\"crtd_date\"]]\n",
    "GA_xml[\"crtd_date\"]=pd.Series(crtd)\n",
    "limit3hr= []\n",
    "for row in GA_xml.itertuples():\n",
    "    if row.crtd_date.minute > 30:\n",
    "        temp= row.crtd_date + timedelta(hours=3)\n",
    "    else:\n",
    "        temp= row.crtd_date + timedelta(hours=2)\n",
    "    limit3hr.append(temp)\n",
    "\n",
    "GA_xml[\"limit\"]= pd.Series(limit3hr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#summing up views for first 3 hours\n",
    "GA3= [] #only relevant rows for next 3 hr views\n",
    "for row in GA_xml.itertuples():\n",
    "    if row.hourofday <= row.limit:\n",
    "        GA3.append(row)\n",
    "\n",
    "GA3=pd.DataFrame(GA3)\n",
    "\n",
    "\n",
    "GA3final=GA3.groupby([\"news_id\", \"crtd_date\", \"limit\"], as_index=False). agg({\"Pageviews\":sum, \"uniquepv\":sum})\n",
    "#GA3final[\"news_id\"]= pd.Series([re.split(\"/\",x)[0] for x in GA3final[\"\"]])\n",
    "\n",
    "#GA3_final has only articles with nonzero views in first three hours. we want all articles with 0 as views count in first 3 hours if no views.\n",
    "GA_xml_grouped=GA_xml.groupby([\"news_id\", \"crtd_date\", \"limit\"], as_index=False).count()\n",
    "GA_xml_grouped.drop(columns=['Page', 'hourofday', 'Pageviews','uniquepv'], axis=1, inplace=True)\n",
    "\n",
    "GA_final_withallids= pd.merge(GA_xml_grouped,GA3final, on= ['news_id', 'crtd_date', 'limit'], how= \"left\")\n",
    "GA_final_withallids.fillna(0, inplace=True)#final data for first 3 hour views for all 87431 articles\n",
    "\n",
    "GA3_final_withallids=pd.read_pickle(\"GA3_final_withallids\")#first 3 hr views\n",
    "GA_final_withallids.to_pickle(\"GA3_final_withallids\")\n",
    "#summing up first 3 days views\n",
    "#pass hourwise dataframe in data here GA_xml-it must have columns Index(['news_id', 'crtd_date', 'Page', 'hourofday', 'Pageviews',\n",
    "      # 'uniquepv', 'month'],\n",
    "def add_ndays_views(data, day):\n",
    "    #limit= []\n",
    "    #for row in data.itertuples():\n",
    "       # temp= row.crtd_date + timedelta(days =3)\n",
    "        #limit.append(temp)\n",
    "\n",
    "    #data[\"limit\"]= pd.Series(limit)\n",
    "    #del limit\n",
    "    data[\"limit\"] = data.apply(lambda row: row[\"crtd_date\"] + timedelta(days=3), axis=1)\n",
    "    # summing up views for first ndays\n",
    "    relevant = []  # only relevant rows for next n days views\n",
    "    for row in data.itertuples():\n",
    "        if row.hourofday <= row.limit:\n",
    "            relevant.append(row)\n",
    "\n",
    "    relevant_data = pd.DataFrame(relevant)\n",
    "    del relevant\n",
    "    relevant_data_grouped = relevant_data.groupby([\"news_id\", \"crtd_date\", \"limit\"], as_index=False).agg({\"Pageviews\": sum, \"uniquepv\": sum})\n",
    "    #zeroviews = set(data[\"news_id\"]) - set(relevant_data[\"news_id\"])\n",
    "    #data.loc[data[\"news_id\"]== 948234].index\n",
    "    #data.loc[1037195, :]checking\n",
    "    data_grouped = data.groupby([\"news_id\", \"crtd_date\", \"limit\"], as_index=False).count()\n",
    "    data_grouped.drop(columns=['Page', 'hourofday', 'Pageviews', 'uniquepv'], axis=1, inplace=True)\n",
    "\n",
    "    final_summed_views = pd.merge(data_grouped, relevant_data_grouped, on=['news_id', 'crtd_date', 'limit'], how=\"left\")\n",
    "    final_summed_views.fillna(0, inplace=True)  # final data for first n days views for all 87431 articles\n",
    "\n",
    "    return  final_summed_views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting number of entities and other entities from content:\n",
    "i1=[\"PERSON\", \"NORP\", \"ORG\", \"GPE\"]#included in number of entities\n",
    "i2=[ \"FAC\", \"LOC\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\"]#included in other  entities\n",
    "import spacy\n",
    "import pandas as pd\n",
    "nlp = spacy.load('en', disable=[\"parser\"])\n",
    "path = '/home/dhruvpatel/PycharmProjects/Virality/Articlespk/*.xlsx'\n",
    "files = glob.glob(path)\n",
    "\n",
    "all_articles = pd.DataFrame() #all articles with article id\n",
    "for f in files:\n",
    "    df = pd.read_excel(f)\n",
    "    print(len(df))\n",
    "    all_articles = all_articles.append(df,ignore_index=True, sort=True)\n",
    "df_list=list(all_articles[\"english\"])\n",
    "# included=[ \"PERSON\", \"NORP\", \"FAC\", \"ORG\", \"GPE\", \"LOC\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\"]\n",
    "# #\"WORK_OF_ART\", \"LAW\", \"LANGUAGE\", \"DATE\", \"TIME\", \"PERCENT\" \"MONEY\", \"QUANTITY\", \"ORDINAL\", \"CARDINAL\",\n",
    "# list_of_entities= []\n",
    "# for i in df_list:\n",
    "#     temp=nlp(i)\n",
    "#     li=[x for x in temp.ents if x.label_  in included]\n",
    "#     list_of_entities.append(li)\n",
    "\n",
    "i1=[\"PERSON\", \"NORP\", \"ORG\", \"GPE\"]\n",
    "i2=[ \"FAC\", \"LOC\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\"]\n",
    "num_of_entities=[]\n",
    "other_entities=[]\n",
    "for i in df_list.itertuples(index=False):\n",
    "    temp=nlp(i)\n",
    "    e1=0\n",
    "    oe=0\n",
    "    for x in temp.ents:\n",
    "        if x.label_ in i1:\n",
    "            e1+=1\n",
    "        if x.label_ in i2:\n",
    "            oe+=1\n",
    "    num_of_entities.append(e1)\n",
    "    other_entities.append(oe)\n",
    "all_articles[\"num_of_entities\"]=pd.Series(\"num_of_entities\")\n",
    "all_articles[\"other_entities\"]=pd.Series(\"other_entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting article category\n",
    "GA_xml= pd.read_pickle(\"GA_xml.pickle\")\n",
    "GA_xml=GA_xml.reset_index()\n",
    "cat=[]\n",
    "for i in range(len(GA_xml)):\n",
    "    try:\n",
    "        temp=re.findall(\"/[a-zA-Z0-9-]*/news/\", GA_xml.Page[i])[0].split(\"/\")[1]\n",
    "        cat.append(temp)\n",
    "    except:\n",
    "        #print(row.Page)\n",
    "        temp=\"na\"\n",
    "        cat.append(temp)\n",
    "len(cat)\n",
    "c=0\n",
    "for i in cat:\n",
    "    if i==\"na\":\n",
    "        c+=1\n",
    "GA_xml[\"category\"]=pd.Series(cat)\n",
    "GA_xml2=GA_xml.loc[GA_xml[\"category\"]!=\"na\", :]\n",
    "GA_xml3=GA_xml2.loc[:,[\"news_id\", \"category\"]]\n",
    "GA_xml3.drop_duplicates(inplace=True)\n",
    "tr=GA_xml3.drop_duplicates(subset='news_id', keep=\"last\")\n",
    "p=tr[\"category\"].value_counts()\n",
    "#a57 diferent categories. we will group them in 5 categories.\n",
    "cate=[]\n",
    "for i in tr[\"category\"]:\n",
    "    if i in [\"national\"]:\n",
    "        cate.append(\"national\")\n",
    "    elif i in [\"himachal-pradesh\", \"uttar-pradesh\",\"punjab\",\"haryana\", \"madhya-pradesh\",\"jalandhar\",\"chandigarh\", \"uttrakhand\", \"ludhiana\", \"amritsar\", \"jharkhand\", \"gurdaspur\", \"firozepur\" ,\"hoshiarpur\",\"bathinda\"\n",
    "               \"patiala\",\"barnala\",\"faridkot\",\"kanpurthala\", \"nawanshahr\",\"ambala\", \"faridabad\", \"panipat\",\"khanna\",\"yamunanagar\",\"sonipat\", \"gurgaon\",\"kurukshetra\",\"karnal\", \"jind\", \"sirsa\" , \"bhiwani\"\n",
    "               \"sirsa\", \"bhiwani\", \"kaithal\", \"rphtak\",\"fatehabad\",\"new-delhi\",\"jammu-kashmir\"]:\n",
    "        cate.append((\"regional\"))\n",
    "    elif i in [\"sports\",\"cricket\"]:\n",
    "        cate.append(\"sports\")\n",
    "    elif i in [\"entertainment\"]:\n",
    "        cate.append(\"entertainment\")\n",
    "    elif i in [\"business\"]:\n",
    "        cate.append(\"business\")\n",
    "    elif i in [\"international\"]:\n",
    "        cate.append(\"international\")\n",
    "    elif i in [\"education-and-jobs\"]:\n",
    "        cate.append(\"education-and-jobs\")\n",
    "    elif i in [\"nari\"]:\n",
    "        cate.append(\"nari\")\n",
    "    elif i in [\"dharm\"]:\n",
    "        cate.append(\"dharm\")\n",
    "    elif i in [\"life-style\"]:\n",
    "        cate.append(\"life-style\")\n",
    "    elif i in [\"blogs\"]:\n",
    "        cate.append(\"blogs\")\n",
    "    else:\n",
    "        cate.append(\"other\")\n",
    "\n",
    "#final data\n",
    "tr.reset_index(inplace=True)\n",
    "tr[\"cat\"]=pd.Series(cate)\n",
    "tr[\"cat\"].value_counts()\n",
    "del tr[\"level_0\"]\n",
    "del tr[\"index\"]\n",
    "del tr[\"category\"]\n",
    "tr.to_pickle(\"category_id.pickle\")\n",
    "fi=pd.read_pickle(\"feed_data5.pickle\")\n",
    "#fi=fi.drop(columns=[\"english\"])\n",
    "final=pd.merge(fi,tr, on=\"news_id\")\n",
    "final[\"weekday\"]=final[\"crtd_date\"].apply(lambda x: x.weekday())# Monday is 0 and Sunday is 6\n",
    "final[\"hour\"]=final[\"crtd_date\"].apply(lambda x: x.hour)\n",
    "final.to_pickle(\"feed_data5.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translation\n",
    "from googletrans import Translator\n",
    "def hi_to_eng(text):\n",
    "    translator = Translator()\n",
    "    temp=translator.translate(text,dest=\"en\")\n",
    "    return temp.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text stats features making\n",
    "from textblob import TextBlob\n",
    "text = \"this movie was piece of shit.\"\n",
    "\n",
    "blob = TextBlob(text)\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "#making list of strings from txt files in articles folder\n",
    "path = '/home/dhruvpatel/PycharmProjects/Virality/Articlespk/*.xlsx'\n",
    "files = glob.glob(path)\n",
    "\n",
    "all_articles = pd.DataFrame() #all articles with article id\n",
    "for f in files:\n",
    "    df = pd.read_excel(f)\n",
    "    print(len(df))\n",
    "    all_articles = all_articles.append(df,ignore_index=True, sort=True)\n",
    "\n",
    "\n",
    "polarity=[]\n",
    "subjectivity=[]\n",
    "for row in all_articles.itertuples(index=True):\n",
    "    blob=TextBlob(row.english)\n",
    "    polarity.append(blob.sentiment.polarity)\n",
    "    subjectivity.append(blob.sentiment.subjectivity)\n",
    "\n",
    "all_articles[\"polarity\"]=pd.Series(polarity)\n",
    "all_articles[\"subjectivity\"]=pd.Series(subjectivity)\n",
    "num_of_sentences=[]\n",
    "num_of_syllables=[]\n",
    "length=[]\n",
    "from textstat.textstat import textstat\n",
    "for row in all_articles.itertuples(index=True):\n",
    "    num_of_sentences.append(textstat.sentence_count(row.english))\n",
    "    #num_of_syllables.append(textstat.syllable_count(row.english))\n",
    "    length.append(len(row.english))\n",
    "\n",
    "all_articles[\"num_of_sentences\"]=pd.Series(num_of_sentences)\n",
    "#all_articles[\"num_of_syllables\"]=pd.Series(num_of_syllables)\n",
    "all_articles[\"length\"]=pd.Series(length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
