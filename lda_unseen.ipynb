{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for unseen article, we will use prebuilt models to make features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import gensim\n",
    "import pickle\n",
    "import spacy\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "from textblob import TextBlob\n",
    "from textstat.textstat import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns dictionary of topics\n",
    "#Pass crtd_date in string format as %Y-%m-%d %H:%M:%S'(ex:\"2018-12-11 00:19:31\")\n",
    "def text_to_features(article_content, crtd_date, category):\n",
    "    def unseen_to_vect(text):\n",
    "        trigram_model=gensim.models.phrases.Phraser.load(\"./lda_model_data/trigram_model\")\n",
    "        bigram_model=gensim.models.phrases.Phraser.load(\"./lda_model_data/bigram_model\")\n",
    "        stop_words = stopwords.words('english')\n",
    "        stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "        #python3 -m spacy download en\n",
    "        #1 time download\n",
    "        nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "        # Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "        def remove_stopwords(token_list):\n",
    "            return [word for word in token_list if word not in stop_words]\n",
    "        def lemmatization(token_list, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "            \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "\n",
    "            doc = nlp(\" \".join(token_list))\n",
    "            return [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
    "\n",
    "\n",
    "        tok=list(gensim.utils.simple_preprocess(str(text), deacc=True))\n",
    "        tok=remove_stopwords(tok)\n",
    "        tok=bigram_model[tok]\n",
    "        tok=trigram_model[tok]\n",
    "        tok=lemmatization(tok)\n",
    "        file = open(\"./lda_model_data/id2word_dict.pickle\", 'rb')\n",
    "        #load  information from that file\n",
    "        id2word = pickle.load(file)\n",
    "        # close the file\n",
    "        file.close()\n",
    "        optimal_model=gensim.models.ldamodel.LdaModel.load(\"./lda_model_data/lda15top\")\n",
    "        vocab=list(id2word.values()) #all vocabulary used in lda model, we need to exclude new words from unseen article so that lda works without errors\n",
    "        def remove_new(lemma_list):\n",
    "            return [word for word in lemma_list if word in vocab]\n",
    "        tok= remove_new(tok)\n",
    "        id_freq=id2word.doc2bow(tok)\n",
    "        return {str(x):y for x,y in dict(optimal_model.get_document_topics(id_freq, minimum_probability=0.0)).items()}\n",
    "\n",
    "\n",
    "    #Pass crtd_date in string format as %Y-%m-%dT%H:%M:%S'(ex:\"2018-12-11 00:19:31\")\n",
    "    def unseen_msaav10(text, crtd_date, upto_days=30):#crtd_date is timedate object, upto_days is integer.\n",
    "        crtd_date=datetime.strptime(crtd_date, '%Y-%m-%d %H:%M:%S')\n",
    "        model=gensim.models.doc2vec.Doc2Vec.load(\"./lda_model_data/doc2vec_model\")\n",
    "        #model.docvecs.most_similar(2, topn=30) #indexes refer to indexes of data.csv file\n",
    "        tups=model.docvecs.most_similar(positive=[model.infer_vector(list(gensim.utils.simple_preprocess(str(text), deacc=True)),alpha=0.025, min_alpha=0.001, steps=55)], topn=80000)\n",
    "        data=pd.read_pickle(\"./lda_model_data/data.pickle\")\n",
    "        tot=0\n",
    "        c=0\n",
    "        #data indexed are in alignment with tags of doc2vec results.\n",
    "        for i in tups:\n",
    "            if crtd_date> data.crtd_date[i[0]] and crtd_date-timedelta(days=upto_days) < data.crtd_date[i[0]]:\n",
    "                tot+=data.upv3day[i[0]]\n",
    "                c+=1\n",
    "            if c==10:\n",
    "                break\n",
    "        return {'msaav10':tot/c, \"weekday\": crtd_date.weekday(), \"hour\":crtd_date.hour}\n",
    "\n",
    "\n",
    "\n",
    "    def stats(text):\n",
    "        op={}\n",
    "        op[\"num_of_sentences\"]=textstat.sentence_count(text)\n",
    "        op[\"length\"]= len(text)\n",
    "        blob=blob = TextBlob(text)\n",
    "        op[\"polarity\"]= blob.sentiment.polarity\n",
    "        op[\"subjectivity\"]=blob.sentiment.subjectivity\n",
    "        nlp = spacy.load('en', disable=[\"parser\"])\n",
    "        i1=[\"PERSON\", \"NORP\", \"ORG\", \"GPE\"]\n",
    "        i2=[ \"FAC\", \"LOC\", \"PRODUCT\", \"EVENT\", \"WORK_OF_ART\"]\n",
    "        temp=nlp(text)\n",
    "        e1=0\n",
    "        oe=0\n",
    "        for x in temp.ents:\n",
    "            if x.label_ in i1:\n",
    "                e1+=1\n",
    "            if x.label_ in i2:\n",
    "                oe+=1\n",
    "        op[\"num_of_identities\"]=e1\n",
    "        op[\"other_entities\"]= oe\n",
    "\n",
    "        return op\n",
    "    \n",
    "    features=unseen_to_vect(article_content)\n",
    "    features.update(unseen_msaav10(article_content,crtd_date))\n",
    "    features.update(stats(article_content))\n",
    "    \n",
    "    if category in [\"national\"]:\n",
    "        features[\"cat\"]=\"national\"\n",
    "    elif category in [\"himachal-pradesh\", \"uttar-pradesh\",\"punjab\",\"haryana\", \"madhya-pradesh\",\"jalandhar\",\"chandigarh\", \"uttrakhand\", \"ludhiana\", \"amritsar\", \"jharkhand\", \"gurdaspur\", \"firozepur\" ,\"hoshiarpur\",\"bathinda\"\n",
    "               \"patiala\",\"barnala\",\"faridkot\",\"kanpurthala\", \"nawanshahr\",\"ambala\", \"faridabad\", \"panipat\",\"khanna\",\"yamunanagar\",\"sonipat\", \"gurgaon\",\"kurukshetra\",\"karnal\", \"jind\", \"sirsa\" , \"bhiwani\"\n",
    "               \"sirsa\", \"bhiwani\", \"kaithal\", \"rphtak\",\"fatehabad\",\"new-delhi\",\"jammu-kashmir\"]:\n",
    "        features[\"cat\"]=\"regional\"\n",
    "    elif category in [\"sports\",\"cricket\"]:\n",
    "        features[\"cat\"]=\"sports\"\n",
    "    elif category in [\"entertainment\"]:\n",
    "        features[\"cat\"]=\"entertainment\"\n",
    "    elif category in [\"business\"]:\n",
    "        features[\"cat\"]=\"business\"\n",
    "    elif category in [\"international\"]:\n",
    "        features[\"cat\"]=\"international\"\n",
    "    elif category in [\"education-and-jobs\"]:\n",
    "        features[\"cat\"]=\"education-and-jobs\"\n",
    "    elif category in [\"nari\"]:\n",
    "        features[\"cat\"]=\"nari\"\n",
    "    elif category in [\"dharm\"]:\n",
    "        features[\"cat\"]=\"dharm\"\n",
    "    elif category in [\"life-style\"]:\n",
    "        features[\"cat\"]=\"life-style\"\n",
    "    elif category in [\"blogs\"]:\n",
    "        features[\"cat\"]=\"blogs\"\n",
    "    else:\n",
    "        features[\"cat\"]=\"other\"\n",
    "\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/psdhruv/python-environments/env3/lib/python3.5/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
      "/home/psdhruv/python-environments/env3/lib/python3.5/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': 0.02268612,\n",
       " '1': 0.028077157,\n",
       " '10': 0.011487752,\n",
       " '11': 0.67987967,\n",
       " '12': 0.016253043,\n",
       " '13': 0.030137703,\n",
       " '14': 0.038450055,\n",
       " '2': 0.013336904,\n",
       " '3': 0.026404655,\n",
       " '4': 0.014157324,\n",
       " '5': 0.021940619,\n",
       " '6': 0.015841777,\n",
       " '7': 0.038382106,\n",
       " '8': 0.011957343,\n",
       " '9': 0.031007748,\n",
       " 'cat': 'sports',\n",
       " 'hour': 10,\n",
       " 'length': 29,\n",
       " 'msaav10': 972.7,\n",
       " 'num_of_identities': 0,\n",
       " 'num_of_sentences': 1,\n",
       " 'other_entities': 0,\n",
       " 'polarity': 0.7,\n",
       " 'subjectivity': 0.6000000000000001,\n",
       " 'weekday': 5}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=\"NEW DELHI: Finance Minister Nirmala Sitharaman presents the first budget of Modi 2.0 government at 11 am today. In February's interim budget, the then acting finance minister Piyush Goyal had provided several income tax sops to the middle-class and introduced zero tax liability for those in the ₹5 lakh income bracket. The interim budget had also increased standard deduction to ₹50,000 from ₹40,000 for the salaried class.\"\n",
    "\n",
    "text_to_features(x, \"2019-05-25 10:10:01\", \"sports\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
