{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import shutil\n",
    "import numpy as np\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_data8=pd.read_pickle(\"feed_data8.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_data8=feed_data8.drop(columns=[\"english\", \"news_id\", \"crtd_date\", \"pv3hr\", \"pv3day\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_data8.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_data8.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=feed_data8.iloc[:70000,:]\n",
    "df_valid=feed_data8.iloc[70000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"./final_data2/df_train.csv\", index=False)\n",
    "df_valid.to_csv(\"./final_data2/df_valid.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid=pd.read_csv(\"./final_data2/df_valid.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fdata_shuffled=feed_data8.sample(frac=1).reset_index(drop= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fdata_shuffled.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_train=fdata_shuffled.iloc[:70000,:]\n",
    "# df_valid=fdata_shuffled.iloc[70000:,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alternate split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train.to_csv(\"./final_data/df_train.csv\", index=False)\n",
    "# df_valid.to_csv(\"./final_data/df_valid.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMNS=['upv3day', 'polarity', 'subjectivity', 'num_of_sentences',\n",
    "       'length', 'num_of_identities', 'other_entities', 'upv3hr', 'msaav10', '0', '1',\n",
    "       '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14',\n",
    "       'cat', 'weekday', 'hour']\n",
    "FEATURES = CSV_COLUMNS[1:]\n",
    "LABEL = CSV_COLUMNS[0]\n",
    "#deFAULTS DEFINES datatype of column in tf.decode_csv\n",
    "DEFAULTS = [[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[0.0],[\"none\"],[0],[0]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename, mode, batch_size = 500):\n",
    "   def _input_fn():\n",
    "      def decode_csv(row):\n",
    "        columns = tf.decode_csv(row, record_defaults = DEFAULTS)\n",
    "        features = dict(zip(CSV_COLUMNS, columns))\n",
    "        label = features.pop(LABEL) # remove label from features and store\n",
    "        return features, label\n",
    "\n",
    "      # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "      filenames_dataset = tf.data.Dataset.list_files(filename, shuffle=False)\n",
    "      # Read lines from text files\n",
    "      textlines_dataset = filenames_dataset.flat_map(tf.data.TextLineDataset).skip(count=1)\n",
    "      # Parse text lines as comma-separated values (CSV)\n",
    "      dataset = textlines_dataset.map(decode_csv)\n",
    "\n",
    "      # Note:\n",
    "      # use tf.data.Dataset.flat_map to apply one to many transformations (here: filename -> text lines)\n",
    "      # use tf.data.Dataset.map      to apply one to one  transformations (here: text line -> feature list)\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "          num_epochs = None # loop indefinitely, so limit will be difined by max steps in estimator\n",
    "          dataset = dataset.shuffle(buffer_size = 70000, seed=2)#100 * batch_size\n",
    "      else:\n",
    "          num_epochs = 1 # end-of-input after this\n",
    "\n",
    "      dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "\n",
    "      return  dataset.make_one_shot_iterator().get_next()\n",
    "   return _input_fn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_train_input_fn():\n",
    "  return read_dataset('./final_data2/df_train.csv', mode = tf.estimator.ModeKeys.TRAIN, batch_size=70000)\n",
    "\n",
    "def get_valid_input_fn():\n",
    "  return read_dataset('./final_data2/df_valid.csv', mode = tf.estimator.ModeKeys.EVAL,batch_size=5000 )\n",
    "\n",
    "def make_feature_cols():#no y here , must enclose cat columns in indentity or embedding cols for dnn\n",
    "    categorical_column1=tf.feature_column.categorical_column_with_vocabulary_list(key='cat', vocabulary_list=('blogs','business','dharm','education-and-jobs','entertainment','international','life-style','nari','national','other','regional','sports'))\n",
    "    categorical_column2=tf.feature_column.categorical_column_with_identity(key='weekday', num_buckets=7)\n",
    "    categorical_column3=tf.feature_column.bucketized_column(source_column=tf.feature_column.numeric_column(\"hour\"), boundaries=[6,12,16,20])\n",
    "    input_columns = [tf.feature_column.numeric_column('polarity'),\n",
    "                     tf.feature_column.numeric_column('subjectivity'),\n",
    "                     tf.feature_column.numeric_column('num_of_sentences'),\n",
    "                     tf.feature_column.numeric_column('length'),\n",
    "                     tf.feature_column.numeric_column('num_of_identities'),\n",
    "                     tf.feature_column.numeric_column('other_entities'),\n",
    "                     tf.feature_column.numeric_column('upv3hr'),\n",
    "                     tf.feature_column.numeric_column('msaav10'),\n",
    "                     tf.feature_column.numeric_column('0'),\n",
    "                     tf.feature_column.numeric_column('1'),\n",
    "                     tf.feature_column.numeric_column('2'),\n",
    "                     tf.feature_column.numeric_column('3'),\n",
    "                     tf.feature_column.numeric_column('4'),\n",
    "                     tf.feature_column.numeric_column('5'),\n",
    "                     tf.feature_column.numeric_column('6'),\n",
    "                     tf.feature_column.numeric_column('7'),\n",
    "                     tf.feature_column.numeric_column('8'),\n",
    "                     tf.feature_column.numeric_column('9'),\n",
    "                     tf.feature_column.numeric_column('10'),\n",
    "                     tf.feature_column.numeric_column('11'),\n",
    "                     tf.feature_column.numeric_column('12'),\n",
    "                     tf.feature_column.numeric_column('13'),\n",
    "                     tf.feature_column.numeric_column('14'),\n",
    "                     tf.feature_column.embedding_column(categorical_column1,dimension=10),\n",
    "                     \n",
    "                     tf.feature_column.indicator_column(categorical_column2),\n",
    "                     tf.feature_column.indicator_column(categorical_column3)]\n",
    "    return input_columns\n",
    "\n",
    "def serving_input_fn():#actual datatype in data for serving\n",
    "    feature_placeholders =  {\n",
    "        column.name: tf.placeholder(tf.float32, [None]) for column in list(make_feature_cols())[:23]\n",
    "    }\n",
    "\n",
    "    feature_placeholders[\"cat\"]=tf.placeholder(tf.string, [None])\n",
    "    feature_placeholders[\"weekday\"]=tf.placeholder(tf.int32, [None])\n",
    "    feature_placeholders[\"hour\"]=tf.placeholder(tf.int32, [None])\n",
    "    \n",
    "    \n",
    "    features = feature_placeholders\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n",
    "\n",
    "\n",
    "def my_metric(labels, predictions):\n",
    "    pred_values = predictions['predictions']\n",
    "    return {'metric1': tf.metrics.root_mean_squared_error(labels, pred_values)}\n",
    "\n",
    "run_config = tf.estimator.RunConfig( save_checkpoints_secs = 5, keep_checkpoint_max = 4, tf_random_seed=100)\n",
    "\n",
    "\n",
    "def train_and_evaluate(num_train_steps):\n",
    "    #tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "    estimator = tf.estimator.DNNRegressor(\n",
    "        config = run_config,\n",
    "        model_dir = \"./cloudmlemodels\",\n",
    "        feature_columns = make_feature_cols(),\n",
    "        hidden_units = [100,90,81,73],\n",
    "        optimizer=tf.train.AdagradOptimizer(learning_rate=0.005))\n",
    "    estimator = tf.contrib.estimator.add_metrics(estimator, my_metric)\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn =get_train_input_fn(),\n",
    "         max_steps = num_train_steps)\n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = get_valid_input_fn(),\n",
    "        steps = None,#none will evaluate whole eval data in each test\n",
    "        start_delay_secs = 10,\n",
    "        throttle_secs = 10,\n",
    "        exporters = exporter)\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.feature_column.numeric_column('msaav10').name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(df_train.iloc[500,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = \"./cloudmlemodels\"\n",
    "\n",
    "\n",
    "# Run training    \n",
    "#shutil.rmtree(OUTDIR, ignore_errors = True) # start fresh each time\n",
    "#tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "train_and_evaluate(80)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=tf.estimator.DNNRegressor(\n",
    "        config = run_config,\n",
    "         model_dir = \"./cloudmlemodels\",\n",
    "         feature_columns = make_feature_cols(),\n",
    "         hidden_units = [100,90,81,73])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(input_fn =get_valid_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_rmse(model, df):\n",
    "  metrics = model.evaluate(input_fn = get_valid_input_fn(df))\n",
    "  print('RMSE on dataset = {}'.format(np.sqrt(metrics['average_loss'])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.contrib.estimator.add_metrics(model, my_metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(input_fn = get_valid_input_fn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_pred=list(model.predict(input_fn = get_valid_input_fn()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(input_fn = get_valid_input_fn())\n",
    "\n",
    "\n",
    "pred=list(predictions)\n",
    "#pred={key:value for key,value in enumerate(pred)}\n",
    "len(pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
